{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 命名实体识别\n",
    "（Named Entity Recognition，简称NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK中的命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker/PY3/english_ace_multiclass.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/huangxinzhe/nltk_data'\n    - '/Users/huangxinzhe/opt/anaconda3/envs/common8/nltk_data'\n    - '/Users/huangxinzhe/opt/anaconda3/envs/common8/share/nltk_data'\n    - '/Users/huangxinzhe/opt/anaconda3/envs/common8/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/huangxinzhe/code/NLP_note/04_ner.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangxinzhe/code/NLP_note/04_ner.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# tag sentences and use nltk's Named Entity Chunker\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangxinzhe/code/NLP_note/04_ner.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m tagged_sentences \u001b[39m=\u001b[39m [nltk\u001b[39m.\u001b[39mpos_tag(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m tokenized_sentences]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huangxinzhe/code/NLP_note/04_ner.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m ne_chunked_sents \u001b[39m=\u001b[39m [nltk\u001b[39m.\u001b[39mne_chunk(tagged) \u001b[39mfor\u001b[39;00m tagged \u001b[39min\u001b[39;00m tagged_sentences]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangxinzhe/code/NLP_note/04_ner.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# extract all named entities\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangxinzhe/code/NLP_note/04_ner.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m named_entities \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m/Users/huangxinzhe/code/NLP_note/04_ner.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangxinzhe/code/NLP_note/04_ner.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# tag sentences and use nltk's Named Entity Chunker\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangxinzhe/code/NLP_note/04_ner.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m tagged_sentences \u001b[39m=\u001b[39m [nltk\u001b[39m.\u001b[39mpos_tag(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m tokenized_sentences]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huangxinzhe/code/NLP_note/04_ner.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m ne_chunked_sents \u001b[39m=\u001b[39m [nltk\u001b[39m.\u001b[39;49mne_chunk(tagged) \u001b[39mfor\u001b[39;00m tagged \u001b[39min\u001b[39;00m tagged_sentences]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangxinzhe/code/NLP_note/04_ner.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# extract all named entities\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huangxinzhe/code/NLP_note/04_ner.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m named_entities \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/common8/lib/python3.8/site-packages/nltk/chunk/__init__.py:183\u001b[0m, in \u001b[0;36mne_chunk\u001b[0;34m(tagged_tokens, binary)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     chunker_pickle \u001b[39m=\u001b[39m _MULTICLASS_NE_CHUNKER\n\u001b[0;32m--> 183\u001b[0m chunker \u001b[39m=\u001b[39m load(chunker_pickle)\n\u001b[1;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m chunker\u001b[39m.\u001b[39mparse(tagged_tokens)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/common8/lib/python3.8/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/common8/lib/python3.8/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/common8/lib/python3.8/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker/PY3/english_ace_multiclass.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/huangxinzhe/nltk_data'\n    - '/Users/huangxinzhe/opt/anaconda3/envs/common8/nltk_data'\n    - '/Users/huangxinzhe/opt/anaconda3/envs/common8/share/nltk_data'\n    - '/Users/huangxinzhe/opt/anaconda3/envs/common8/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "def parse_document(document):\n",
    "   document = re.sub('\\n', ' ', document)\n",
    "   if isinstance(document, str):\n",
    "       document = document\n",
    "   else:\n",
    "       raise ValueError('Document is not string!')\n",
    "   document = document.strip()\n",
    "   sentences = nltk.sent_tokenize(document)\n",
    "   sentences = [sentence.strip() for sentence in sentences]\n",
    "   return sentences\n",
    "\n",
    "# sample document\n",
    "text = \"\"\"\n",
    "FIFA was founded in 1904 to oversee international competition among the national associations of Belgium, \n",
    "Denmark, France, Germany, the Netherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich, its \n",
    "membership now comprises 211 national associations. Member countries must each also be members of one of \n",
    "the six regional confederations into which the world is divided: Africa, Asia, Europe, North & Central America \n",
    "and the Caribbean, Oceania, and South America.\n",
    "\"\"\"\n",
    "\n",
    "# tokenize sentences\n",
    "sentences = parse_document(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "# tag sentences and use nltk's Named Entity Chunker\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "ne_chunked_sents = [nltk.ne_chunk(tagged) for tagged in tagged_sentences]\n",
    "# extract all named entities\n",
    "named_entities = []\n",
    "for ne_tagged_sentence in ne_chunked_sents:\n",
    "   for tagged_tree in ne_tagged_sentence:\n",
    "       # extract only chunks having NE labels\n",
    "       if hasattr(tagged_tree, 'label'):\n",
    "           entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #get NE name\n",
    "           entity_type = tagged_tree.label() # get NE category\n",
    "           named_entities.append((entity_name, entity_type))\n",
    "           # get unique named entities\n",
    "           named_entities = list(set(named_entities))\n",
    "\n",
    "# store named entities in a data frame\n",
    "entity_frame = pd.DataFrame(named_entities, columns=['Entity Name', 'Entity Type'])\n",
    "# display results\n",
    "print(entity_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
