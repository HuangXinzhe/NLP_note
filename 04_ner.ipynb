{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 命名实体识别\n",
    "（Named Entity Recognition，简称NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK中的命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/huangxinzhe/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/huangxinzhe/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/huangxinzhe/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/huangxinzhe/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Entity Name   Entity Type\n",
      "0         Caribbean      LOCATION\n",
      "1           Belgium           GPE\n",
      "2            Europe           GPE\n",
      "3            Sweden           GPE\n",
      "4            Africa        PERSON\n",
      "5     South America           GPE\n",
      "6       Switzerland           GPE\n",
      "7           Oceania           GPE\n",
      "8   Central America  ORGANIZATION\n",
      "9           Germany           GPE\n",
      "10           France           GPE\n",
      "11            Spain           GPE\n",
      "12      Netherlands           GPE\n",
      "13           Zürich           GPE\n",
      "14          Denmark           GPE\n",
      "15            North           GPE\n",
      "16             Asia           GPE\n",
      "17             FIFA  ORGANIZATION\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "def parse_document(document):\n",
    "   document = re.sub('\\n', ' ', document)\n",
    "   if isinstance(document, str):\n",
    "       document = document\n",
    "   else:\n",
    "       raise ValueError('Document is not string!')\n",
    "   document = document.strip()\n",
    "   sentences = nltk.sent_tokenize(document)\n",
    "   sentences = [sentence.strip() for sentence in sentences]\n",
    "   return sentences\n",
    "\n",
    "# sample document\n",
    "text = \"\"\"\n",
    "FIFA was founded in 1904 to oversee international competition among the national associations of Belgium, \n",
    "Denmark, France, Germany, the Netherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich, its \n",
    "membership now comprises 211 national associations. Member countries must each also be members of one of \n",
    "the six regional confederations into which the world is divided: Africa, Asia, Europe, North & Central America \n",
    "and the Caribbean, Oceania, and South America.\n",
    "\"\"\"\n",
    "\n",
    "# tokenize sentences\n",
    "sentences = parse_document(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "# tag sentences and use nltk's Named Entity Chunker\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "ne_chunked_sents = [nltk.ne_chunk(tagged) for tagged in tagged_sentences]\n",
    "# extract all named entities\n",
    "named_entities = []\n",
    "for ne_tagged_sentence in ne_chunked_sents:\n",
    "   for tagged_tree in ne_tagged_sentence:\n",
    "       # extract only chunks having NE labels\n",
    "       if hasattr(tagged_tree, 'label'):\n",
    "           entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #get NE name\n",
    "           entity_type = tagged_tree.label() # get NE category\n",
    "           named_entities.append((entity_name, entity_type))\n",
    "           # get unique named entities\n",
    "           named_entities = list(set(named_entities))\n",
    "\n",
    "# store named entities in a data frame\n",
    "entity_frame = pd.DataFrame(named_entities, columns=['Entity Name', 'Entity Type'])\n",
    "# display results\n",
    "print(entity_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stanford ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "def parse_document(document):\n",
    "   document = re.sub('\\n', ' ', document)\n",
    "   if isinstance(document, str):\n",
    "       document = document\n",
    "   else:\n",
    "       raise ValueError('Document is not string!')\n",
    "   document = document.strip()\n",
    "   sentences = nltk.sent_tokenize(document)\n",
    "   sentences = [sentence.strip() for sentence in sentences]\n",
    "   return sentences\n",
    "\n",
    "# sample document\n",
    "text = \"\"\"\n",
    "FIFA was founded in 1904 to oversee international competition among the national associations of Belgium, \n",
    "Denmark, France, Germany, the Netherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich, its \n",
    "membership now comprises 211 national associations. Member countries must each also be members of one of \n",
    "the six regional confederations into which the world is divided: Africa, Asia, Europe, North & Central America \n",
    "and the Caribbean, Oceania, and South America.\n",
    "\"\"\"\n",
    "\n",
    "sentences = parse_document(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# set java path in environment variables\n",
    "java_path = r'C:\\Program Files\\Java\\jdk1.8.0_161\\bin\\java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "# load stanford NER\n",
    "sn = StanfordNERTagger('E://stanford-ner-2018-10-16/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "                       path_to_jar='E://stanford-ner-2018-10-16/stanford-ner.jar')\n",
    "\n",
    "# tag sentences\n",
    "ne_annotated_sentences = [sn.tag(sent) for sent in tokenized_sentences]\n",
    "# extract named entities\n",
    "named_entities = []\n",
    "for sentence in ne_annotated_sentences:\n",
    "   temp_entity_name = ''\n",
    "   temp_named_entity = None\n",
    "   for term, tag in sentence:\n",
    "       # get terms with NE tags\n",
    "       if tag != 'O':\n",
    "           temp_entity_name = ' '.join([temp_entity_name, term]).strip() #get NE name\n",
    "           temp_named_entity = (temp_entity_name, tag) # get NE and its category\n",
    "       else:\n",
    "           if temp_named_entity:\n",
    "               named_entities.append(temp_named_entity)\n",
    "               temp_entity_name = ''\n",
    "               temp_named_entity = None\n",
    "\n",
    "# get unique named entities\n",
    "named_entities = list(set(named_entities))\n",
    "# store named entities in a data frame\n",
    "entity_frame = pd.DataFrame(named_entities, columns=['Entity Name', 'Entity Type'])\n",
    "# display results\n",
    "print(entity_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
